<context>
# Overview
The Agentic OPS Assistant POC is an internal tool designed for OPS Management, Resource Planners, and Practice/Competency Leads. It addresses the limitations of current static reporting methods by employing an agentic system architecture, implemented using the open-source Microsoft AutoGen framework, leveraging open-source tools and powered by a GPT Large Language Model (LLM) via OpenAI's API for advanced reasoning where applicable. This system analyzes operational data stored and processed using SQLite, an open-source database, to provide proactive insights via a Streamlit interface.
The POC solves problems of reactive reporting and manual data correlation by automating data integration into the SQLite database and using specialized AutoGen agents, potentially leveraging GPT model capabilities, to:
- Continuously monitor performance against dimension-based monthly targets by querying the SQLite database.
- Predict potential target misses and associated project risks before they become critical.
- Provide actionable recommendations, potentially enhanced by LLM reasoning, based on integrated data analysis.
- Enable interactive "what-if" scenario simulation using current and historical data context from the SQLite database.
- Automatically highlight trends (including Month-over-Month comparisons) and deviations from plans by analyzing data persisted over time in the SQLite database.
The value lies in transforming operational management via an autonomous, collaborating agent system built on an open-source foundation (AutoGen, Python, SQLite, Streamlit), augmented by the reasoning power of GPT models. This POC aims to prove that this approach provides value significantly superior to existing methods. User interaction in this POC is primarily through the Streamlit dashboard elements, receiving insights generated by the agents, rather than direct free-form conversation.

# Core Features
- **Automated Performance Monitoring & Alerting:**
    - *What it does:* Provides automatic, near real-time views of performance against targets, highlighting deviations and trends (MoM) proactively.
    - *Why it's important:* Reduces manual effort in tracking performance and surfaces potential issues faster.
    - *How it works:* A Monitoring Agent queries the SQLite DB for actuals vs targets, calculates trends, identifies deviations based on rules, and generates alerts.
- **Predictive Risk Identification:**
    - *What it does:* Forecasts potential future target misses based on current trends.
    - *Why it's important:* Allows managers to take corrective action before problems become critical.
    - *How it works:* The Monitoring Agent uses simple forecasting techniques on historical data from the SQLite DB.
- **Targeted Actionable Recommendations:**
    - *What it does:* Generates specific, context-aware suggestions to address identified risks or deviations.
    - *Why it's important:* Helps users quickly identify potential solutions instead of just seeing problems.
    - *How it works:* A Recommendation Agent analyzes alert context, queries the SQLite DB for details (availability, project needs), applies rules (potentially LLM-enhanced), and suggests actions.
- **Interactive "What-If" Scenario Simulation:**
    - *What it does:* Allows users to model the impact of potential resource allocation changes on key metrics via simple UI inputs.
    - *Why it's important:* Enables data-driven decision-making regarding resource movements.
    - *How it works:* A Simulation Agent takes user parameters from the UI, queries the SQLite DB for baseline, applies changes, recalculates projections using tool functions, and presents results.
- **Focused Operational Dashboard:**
    - *What it does:* Presents key insights, alerts, recommendations, and simulation results in a clear, consolidated Streamlit interface.
    - *Why it's important:* Provides a single pane of glass for operational oversight derived from the agent system.
    - *How it works:* The Streamlit UI receives structured data from the agents (via the User Proxy Agent) and displays it using appropriate widgets and visualizations.
- **Automated Data Integration:**
    - *What it does:* Handles the ingestion and processing of source data files into the SQLite database.
    - *Why it's important:* Ensures the data foundation for the agents is up-to-date and correctly structured.
    - *How it works:* A Python script uses Pandas to read source files (CSV/XLSX), performs transformations, and loads data into the SQLite DB using its connector library.

# User Experience
- **User Personas:** OPS Manager, Resource Planner, Practice/Competency Lead.
- **Key User Flows:**
    - Reviewing the dashboard for overall status and active alerts.
    - Drilling into specific alerts generated by the Monitoring Agent.
    - Evaluating recommendations provided by the Recommendation Agent.
    - Using Streamlit forms/widgets to define parameters for a "what-if" simulation.
    - Reviewing the simulation results presented by the Simulation Agent.
    - Filtering/segmenting the dashboard view based on dimensions (Competency, Location, etc.).
- **UI/UX Considerations:** Simplicity, clarity of information (especially alerts and recommendations), action-oriented design using Streamlit widgets, fast feedback for simulations, minimal learning curve.
</context>

<PRD>
# Technical Architecture
- **System Components:**
    - **Data Ingestion Module (Python, Pandas, sqlite3/SQLAlchemy):** Handles reading source files (CSV/XLSX), cleaning/transforming data, and loading into the SQLite database according to the defined schema.
    - **SQLite Database:** Stores processed data from sources (Charged Hours, Master File, MLP, Targets).
    - **AutoGen Agents:** Defined within the AutoGen framework:
        - **Monitoring_Agent (AssistantAgent):**
            - *Functionality:* Periodically (or on trigger) queries the SQLite database (via tools) for actual performance data (current & historical). Compares actuals against dimensional targets. Calculates MoM changes and other trends. Performs simple forecasting to predict potential future target deviations. Identifies significant deviations/risks based on predefined rules/thresholds. Formats findings into structured alert messages. Correlates segment performance issues with potentially impacted projects (using MLP data from DB). May trigger the Recommendation Agent.
            - *Tools:* Requires Python functions capable of executing parameterized SQL queries against SQLite to fetch aggregated segment performance, targets, project data, and historical data for trends/MoM comparisons.
        - **Recommendation_Agent (AssistantAgent):**
            - *Functionality:* Receives alert context (likely from Monitoring Agent or an orchestrator). Queries SQLite database (via tools) for deeper context (e.g., specific employee availability within a segment, detailed project needs from MLP, skills from Master File). Applies a predefined set of rules, potentially augmented by GPT reasoning on the context, to generate 1-2 specific, actionable recommendations (e.g., "Suggest reallocating [X FTE] of [Competency Y] from Project A to Project B", "Investigate low utilization for [Segment Z]"). Formats recommendations clearly.
            - *Tools:* Requires Python functions capable of executing targeted SQL queries against SQLite for resource availability, skill matching (basic), and project details.
        - **Simulation_Agent (AssistantAgent):**
            - *Functionality:* Receives simulation parameters from the user via the User Proxy Agent (e.g., "Simulate moving 1 FTE of Competency X from Project A to Project B for next month"). Queries SQLite (via tools) for the current baseline state. Applies the hypothetical change to the relevant data points. Recalculates projected outcomes (e.g., impact on segment utilization targets for the affected month, impact on projected project completion metrics). Returns a comparison of the baseline vs. simulated outcome. Primarily relies on deterministic calculations via tool functions, but might use GPT to interpret results/explain impacts.
            - *Tools:* Requires Python functions to query SQLite for baseline data and perform the recalculation logic based on simulation inputs.
        - **User_Proxy_Agent (UserProxyAgent):**
            - *Functionality:* Represents the human user interacting via the Streamlit interface. Translates user actions (button clicks, form submissions in Streamlit) into instructions or prompts for other agents (primarily the Simulation Agent for triggering simulations). Receives responses/results from other agents and passes them back to the Streamlit UI for display. Does not require direct LLM interaction itself but facilitates communication.
    - **Tool Functions (Python):** Encapsulate database interactions (SQL queries against SQLite) and deterministic calculations (forecasting, simulation logic). Crucial for grounding agent responses in real data and ensuring reliability.
    - **Web UI Server:** Streamlit application serving the dashboard interface.
- **Data Models:** Conceptual SQLite Schema derived from data requirements (details in Appendix). Schema diagram to be added.
- **APIs and Integrations:**
    - OpenAI API (for GPT model access by AssistantAgents).
    - Internal data sources accessed via file transfer (CSV/XLSX).
    - SQLite Database connection via Python library (sqlite3/SQLAlchemy).
- **Infrastructure Requirements (POC):**
    - Python execution environment (local machine or server).
    - Access to source data files.
    - SQLite database file storage.
    - Internet connectivity for OpenAI API calls.
    - Network access for users to reach the Streamlit application.
    - Secure storage for OpenAI API key.
- **Open Source Commitment:** Core frameworks/libraries (Python, AutoGen, Pandas, NumPy, SQLite, Streamlit) are open source; LLM access via commercial OpenAI API.

# Development Roadmap
- **Phase 1 (MVP Foundation):**
    - Setup basic project structure (Python environment, Git repo).
    - Implement Data Ingestion Module for all source files.
    - Define and create SQLite database schema.
    - Develop core Tool Functions for basic data retrieval (performance, targets).
    - Implement Monitoring Agent (basic functionality: fetch data, compare vs targets, basic deviation check).
    - Implement User Proxy Agent and basic Streamlit UI to display Monitoring Agent's output (e.g., simple table/KPIs).
- **Phase 2 (MVP Core Logic):**
    - Enhance Monitoring Agent: Add MoM calculations, trend analysis, simple forecasting, correlation with projects, structured alert generation.
    - Implement Recommendation Agent: Define rule-based recommendations, implement Tools for contextual data retrieval, integrate triggering from Monitoring Agent alerts.
    - Enhance Streamlit UI: Display alerts clearly, show generated recommendations.
- **Phase 3 (MVP Interactivity & Polish):**
    - Implement Simulation Agent: Develop Tools for simulation logic and data retrieval.
    - Integrate Simulation Agent with User Proxy Agent and Streamlit UI: Add forms/widgets for simulation input, display baseline vs. simulation results.
    - Refine visualizations (charts for trends, KPIs).
    - Add filtering capabilities to the Streamlit UI.
    - Basic error handling and logging.
- **Future Enhancements (Post-POC Scope):**
    - Integration with live data sources (DB connections, APIs).
    - More advanced predictive models (ML-based forecasting).
    - Enhanced LLM reasoning for more nuanced recommendations or anomaly detection.
    - User customization (saving views, adjusting thresholds, defining custom segments).
    - Conversational interaction for querying or triggering actions.
    - Role-based access control and user management.
    - Integration with other operational tools (e.g., ticketing systems, resource management platforms).
    - Production deployment architecture (e.g., containerization, cloud hosting).
    - Feedback loop for users to rate recommendation quality.

# Logical Dependency Chain
1.  **Data Foundation:** Ingestion and SQLite Schema must come first. Cannot analyze data that isn't loaded correctly. (Foundation)
2.  **Basic Monitoring & Display:** Get data visible in the Streamlit UI via the Monitoring Agent (even if basic) to provide early feedback and validate the data pipeline. (Getting to usable/visible front end)
3.  **Core Analysis Logic:** Build out the analytical capabilities of the Monitoring Agent (trends, forecasts) and the Recommendation Agent (rules, context). (Atomic feature scoping, built upon foundation)
4.  **Interactive Simulation:** Develop the Simulation Agent and its UI integration, as this relies on having baseline data and potentially context from the other agents. (Atomic feature scoping, built upon foundation)
5.  **UI Refinements:** Enhance visualizations and interactivity once core logic is functional. (Improvement upon existing features)

# Risks and Mitigations
- **Technical Challenges:**
    - *Risk:* Data Quality & Consistency issues in source files.
    - *Mitigation:* Implement robust validation checks during ingestion, log errors clearly, establish a feedback loop with data providers.
    - *Risk:* Database Schema Evolution required during development.
    - *Mitigation:* Use a lightweight migration approach if needed (e.g., scripts), document schema changes clearly.
    - *Risk:* Query Performance degradation on SQLite with growing data/complexity.
    - *Mitigation:* Apply database indexing strategically, optimize SQL queries, accept potential performance limits for POC scope, profile queries if issues arise.
    - *Risk:* AutoGen Learning Curve and framework complexity.
    - *Mitigation:* Start with simpler agent interaction patterns, leverage AutoGen documentation and examples, conduct internal knowledge sharing sessions, iterative development.
    - *Risk:* Effective Prompt Engineering for GPT agents.
    - *Mitigation:* Iterative prompt design and testing, use few-shot examples, clearly define expected output format, emphasize tool use for factual grounding, implement validation logic for LLM outputs.
    - *Risk:* LLM Reliability/Consistency (non-determinism, API errors).
    - *Mitigation:* Implement retry mechanisms for API calls with exponential backoff, design agents to be resilient (use tools for critical data), include error handling for API failures, potentially add fallback logic (e.g., rule-based only if LLM fails).
    - *Risk:* LLM Costs and API Availability.
    - *Mitigation:* Optimize token usage (concise prompts, context summarization), monitor API costs via OpenAI dashboard, set budget alerts, secure API keys properly, choose cost-effective GPT model tier suitable for tasks, plan for potential API downtime (though core functionality might degrade).
- **Figuring out the MVP that we can build upon:**
    - *Risk:* Scope Creep or unclear MVP definition.
    - *Mitigation:* Adhere strictly to the phased Development Roadmap, prioritize features delivering core value (monitoring, recommendations, simulation), use the "Out of Scope" list as a boundary, conduct regular stakeholder reviews to confirm alignment.
- **Resource Constraints:**
    - *Risk:* Insufficient developer time or expertise.
    - *Mitigation:* Clearly define scope within the phased roadmap, adjust scope if necessary based on velocity, focus on leveraging open-source components effectively, ensure necessary skills (Python, AutoGen, basic SQL) are available.
- **Data Specific Risks:**
    - *Risk:* Accuracy or interpretation issues with the Target File.
    - *Mitigation:* Early validation of target definitions and dimensions with stakeholders, ensure clear understanding of how targets map to source data.
    - *Risk:* Timeliness of Source Data impacting POC relevance.
    - *Mitigation:* Define and agree on expected data refresh frequency for the POC duration, document the data cut-off dates used for analysis.

# Out of Scope (for POC)
- Real-time data streaming / sub-daily updates.
- User management / Authentication / Authorization.
- Automated data source connections (manual file placement assumed).
- Direct free-form conversational interface (UI interaction is via predefined Streamlit widgets/forms).
- Complex, multi-turn conversational planning between agents requiring deep memory/state management beyond single tasks.
- Production-grade deployment infrastructure (scalability, high availability, automated monitoring).
- Sophisticated ML models beyond simple forecasting implemented in tools.
- User ability to define custom targets or rules *within* the UI.

# Future Enhancements (Post-POC)
- Integration with live data sources (DB connections, APIs).
- More advanced predictive models (ML-based forecasting).
- Enhanced LLM reasoning for more nuanced recommendations or anomaly detection.
- User customization (saving views, adjusting thresholds, defining custom segments).
- Conversational interaction for querying or triggering actions.
- Role-based access control and user management.
- Integration with other operational tools (e.g., ticketing systems, resource management platforms).
- Production deployment architecture (e.g., containerization, cloud hosting).
- Feedback loop for users to rate recommendation quality.

# Appendix
- **Detailed Data Requirements:**
    - **6.1. Source Files Assumption:** Reasonably clean CSV or XLSX files provided periodically.
    - **6.2. Key Identifiers for Joining:**
        - Employee Identifier (e.g., Email, Employee ID) - Master File <> Charged Hours.
        - Project Identifier (e.g., Project Code, Engagement ID) - MLP <> Charged Hours.
        - Year, Month - Charged Hours (aggregated) <> Target File.
        - Employee Category, Employee Competency, Employee Location, Employee Billing Rank - Master File <> Target File (for joining employee data to relevant targets).
    - **6.3. Relevant Columns for Analysis (POC Focus):**
        - *Source 1: Charged Hours Report*
            - Employee Identifier (Key)
            - Employee Name
            - Project Identifier (Key)
            - Project Name
            - Date Worked (-> Year, Month)
            - Charged Hours (Core Metric)
        - *Source 2: Master File (Employee Data)*
            - Employee Identifier (Key)
            - Employee Name
            - Status (Active/Inactive - Critical)
            - Standard Hours Per Week / Effective STD Hrs per Week (Critical Capacity Metric)
            - Employee Category (Key dimension for Targets)
            - Employee Competency / Primary Skill Group (Key dimension for Targets)
            - Employee Location / Office (Key dimension for Targets)
            - Employee Billing Rank / Level / Grade (Key dimension for Targets)
            - Department / Practice Area (Context)
            - Region (Context)
        - *Source 3: MLP (Master List of Projects - Context File)*
            - Project Identifier (Key)
            - Project Name
            - Project Status (Critical)
            - Project Start Date
            - Project End Date / Overall Deadline Date (Critical)
            - Total Budgeted Hours (Critical)
            - Required Primary Skill Category (Context for recommendations)
            - Target Resource Count (FTE) (Overall project need context)
        - *Source 4: Target File (Dimensional Monthly Targets - NEW)*
            - Dimensions (Composite Key): Target Year, Target Month, Employee Category, Employee Competency, Employee Location, Employee Billing Rank
            - Target Metrics: Target Utilization Percentage (Primary), Target Charged Hours per FTE (Optional), Target Headcount (FTE) (Optional)
- **Conceptual SQLite Schema Diagram:** (To be added - based on Data Requirements above).
- **Key Assumptions:**
    - Availability of required operational data in specified formats.
    - Clearly defined and consistent performance targets in the Target File.
    - Access to OpenAI API keys with appropriate usage limits/billing enabled is provisioned.
    - Stakeholder availability for feedback during POC development.
    - Basic Python/AutoGen development environment can be set up.
- **Target Definition Clarifications:** (Reference specific business rules document if available, e.g., how utilization is calculated based on Charged Hours and Standard Hours).
- **AutoGen Specifics:** (Detailed agent prompts, specific tool function signatures - can be added as developed or referenced in separate technical design documents).
- **Success Metrics (POC Evaluation Criteria):** [Define specific, measurable goals here]
    - *Example:* Monitoring agent identifies >80% of significant deviations (as defined by stakeholders) compared to manual analysis within 4 hours of data refresh.
    - *Example:* Recommendations generated are deemed 'actionable' or 'relevant' by >70% of target users surveyed post-POC.
    - *Example:* Simulation results for standard scenarios are produced within 15 seconds and calculations are validated as accurate.
    - *Example:* POC successfully deployed to a shared environment and usable by the 3 defined user personas by the end of the POC timeline.
- **Security Considerations (POC):**
    - OpenAI API keys must be stored securely (e.g., environment variables, secrets management tool) and NOT committed to source control.
    - Access to the running POC application (Streamlit interface) should be limited to intended internal users (mechanism TBD - e.g., IP restriction, simple password protection via Streamlit secrets, internal network hosting).
    - Input data (CSV/XLSX) should be handled according to internal data privacy and security policies. No sensitive PII beyond what's necessary (e.g., Employee ID) should be persisted if avoidable.
</PRD>